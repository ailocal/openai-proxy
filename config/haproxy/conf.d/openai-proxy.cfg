# OpenAI API Proxy Configuration
# Install in: /etc/haproxy/conf.d/openai-proxy.cfg

global
    log stdout local0 notice
    maxconn 4096
    daemon

defaults
    log     global
    mode    http
    option  httplog
    option  http-server-close
    option  http-pretend-keepalive
    timeout connect 5000
    timeout client  50000
    timeout server  50000

frontend openai_proxy
    bind 127.0.0.1:2020
    mode http

    # Set required proxy headers
    http-request set-header X-Forwarded-Proto https if { ssl_fc }
    http-request set-header X-Forwarded-Port %[dst_port]
    http-request set-header X-Forwarded-For %[src]

    # Serve welcome page for root path
    acl is_root path /
    http-response set-header Content-Type text/html if is_root
    http-response return file /etc/haproxy/pages/welcome.http if is_root

    # Route based on path
    acl path_audio_transcriptions path_beg /v1/audio/transcriptions
    acl path_chat_completions path_beg /v1/chat/completions
    acl path_audio_speech path_beg /v1/audio/speech
    acl path_v1 path_beg /v1

    use_backend backend_audio_transcriptions if path_audio_transcriptions
    use_backend backend_chat_completions if path_chat_completions
    use_backend backend_audio_speech if path_audio_speech
    use_backend backend_openai if path_v1

    # Default to welcome page for non-v1 paths
    http-response set-header Content-Type text/html if !path_v1
    http-response return file /etc/haproxy/pages/welcome.http if !path_v1

# Audio Transcriptions Backend
backend backend_audio_transcriptions
    mode http
    option forwardfor
    server openai api.openai.com:443 ssl verify none sni str(api.openai.com)
    
    # Local Whisper Example (uncomment to use)
    # server whisper localhost:9000
    #   - Supports: Whisper models for audio transcription
    #   - Install: pip install whisper-api
    #   - Run: whisper-api --port 9000

# Chat Completions Backend
backend backend_chat_completions
    mode http
    option forwardfor
    server openai api.openai.com:443 ssl verify none sni str(api.openai.com)
    
    # Local Ollama Example (uncomment to use)
    # server ollama localhost:11434
    #   - Supports: llama2, codellama, mistral and other models
    #   - Install: curl https://ollama.ai/install.sh | sh
    #   - Run: ollama serve

# Audio Speech Backend
backend backend_audio_speech
    mode http
    option forwardfor
    server openai api.openai.com:443 ssl verify none sni str(api.openai.com)
    
    # Local TTS Example (uncomment to use)
    # server tts localhost:8880
    #   - Supports: Multiple voices and languages
    #   - Install: pip install kokoro-tts
    #   - Run: kokoro-tts serve --port 8880

# Default OpenAI Backend
backend backend_openai
    mode http
    option forwardfor
    server openai api.openai.com:443 ssl verify none sni str(api.openai.com)
